<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Bert</title>
      <link href="/2022/09/09/bert/"/>
      <url>/2022/09/09/bert/</url>
      
        <content type="html"><![CDATA[<h1 id="什么是Bert？"><a href="#什么是Bert？" class="headerlink" title="什么是Bert？"></a>什么是Bert？</h1><p>BERT（Bidirectional Encoder Representations from Transformers） 是一个语言表示模型(language representation model)。它的主要模型结构是trasnformer的encoder堆叠而成，它的框架分为两个步骤：pretraining和fine-tuning。</p><h1 id="Bert可以做什么？"><a href="#Bert可以做什么？" class="headerlink" title="Bert可以做什么？"></a>Bert可以做什么？</h1><p>首先结论是：Bert有分词输出和序列最后一层hidden的输出。因此Bert的应用在分类任务、回归任务和序列任务中。其中回归任务是一种特殊的分类，只不过它的输出是一个数值而非一个概率。序列任务有命名实体识别(NER)，SQuAD。</p><p>Bert的这种预训练模型使得模型只需要少数的训练数据即可得到优秀的效果。</p><h1 id="Bert模型"><a href="#Bert模型" class="headerlink" title="Bert模型"></a>Bert模型</h1><p>简单来讲Bert分为：输入层，中间层，输出层。</p><h2 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h2><p><img src="1.png"><br>可以看到输入为三个embedding相加。</p><h2 id="中间层"><a href="#中间层" class="headerlink" title="中间层"></a>中间层</h2><p>中间层类似于transformer的encoder<br><img src="2.png"></p><h2 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h2><p>每一个输入对应一个输出，根据我们的不同任务来选择具体的输出。</p><h2 id="Pretraining＋fine-tuning"><a href="#Pretraining＋fine-tuning" class="headerlink" title="Pretraining＋fine-tuning"></a>Pretraining＋fine-tuning</h2><p>我们已经知道Bert的两阶段工作性质了，那么它具体如何工作呢？</p><p><strong>- Pretraining</strong><br>      模型的预训练采用大量的预训练数据，进行unsupervised learning。<br>      Bert采用了MLM的方法：mask language model，随机mask输入的词，由于fine-tuning时       没有mask的token，因此：<br>      1. 80%替换为MASK<br>      2. 10%替换为随机token<br>      3. 10%不变<br><strong>- fine-tuning</strong><br>       在fine-tuning阶段可以简单的选择特定的输入和输出进行训练，如：<br>       - 2句子 pairs： 相似度任务,<br>       - 假设-前提 pairs： 推理任务,<br>       - 问题-文章 pairs ： QA任务</p><hr>]]></content>
      
      
      <categories>
          
          <category> Pretrain model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Pretrain </tag>
            
            <tag> Bert </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prompt Learning</title>
      <link href="/2022/09/09/prompt-learning/"/>
      <url>/2022/09/09/prompt-learning/</url>
      
        <content type="html"><![CDATA[<p>相关论文：Pre-train, Prompt, and Predict:A Systematic Survey of Prompting Methods in Natural Language Processing</p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>在很长一段时间里，NLP任务采用<strong>Pretrain＋fine-tuning</strong>的方法，这种方法的问题是，对于每个NLP任务我们均需要重新fine-tuning一个新的model。这样来看，它的使用效率并不是很高：对特定任务有效，不能够互相共用。那么，是否能够有一个<strong>预训练模型作为所有任务的基础，仅仅需要将不同任务配适到这个预训练模型上来完成下游任务</strong>？</p><p>那么<strong>Prompt Learning</strong>就是上述的配适器，有了它就可以进行高效的预训练模型使用了。<br><img src="1.png"></p><p>如图所示：</p><ul><li>左侧为Model Tuning：对于不同的任务需要将整个预训练模型调整，拥有自己独立的参数。</li><li>右侧为Prompt Tuning：对于不同的任务，仅需要插入不同的prompt参数，每个任务独立拥有自己的prompt参数进行训练，而不对预训练模型重新训练，大大缩短了训练时间，提升了模型的使用率。</li></ul><h2 id="Prompt"><a href="#Prompt" class="headerlink" title="Prompt"></a>Prompt</h2><p>什么是prompt呢？</p><p>在NLP中prompt代表了一个针对<strong>预训练模型的提示</strong>，帮助它理解问题。<br><img src="2.png"></p><p>在原文的例子里可以很好地理解：BERT&#x2F;BART&#x2F;ERNIE 均为预训练语言模型，对于人类提出的问题，以及线索，预训练语言模型可以给出正确的答案。</p><p>Prompt的定义：</p><blockquote><p>Prompt is the technique of making better use of the knowledge from the pre-trained model by adding additional texts to the input.<br>Prompt 是一种为了更好的使用预训练语言模型的知识，采用在输入段添加额外的文本的技术。</p></blockquote><h2 id="Prompt的工作"><a href="#Prompt的工作" class="headerlink" title="Prompt的工作"></a>Prompt的工作</h2><ul><li>Prompt 模版（Template）的构造</li><li>Prompt 答案空间映射（Verbalizer）的构造</li><li>文本代入template，并且使用预训练语言模型进行预测</li><li>将预测的结果映射回label。</li></ul><p><img src="3.png"></p><h3 id="1-prompt-construction"><a href="#1-prompt-construction" class="headerlink" title="1:prompt construction"></a>1:prompt construction</h3><p>首先我们需要构建一个模版Template，模版的作用是将输入和输出进行重新构造，变成一个新的带有mask slots的文本，具体如下：</p><ul><li>定义一个模版，包含了2处代填入的slots：[x] 和 [z]</li><li>将[x] 用输入文本代入</li></ul><p> 如：</p><ul><li>输入：x &#x3D; 我喜欢这个电影。</li><li>模版：[x]总而言之，它是一个[z]电影。</li><li>代入（prompting）：我喜欢这个电影。总而言之，它是一个[z]电影。</li></ul><h3 id="2-answer-construction"><a href="#2-answer-construction" class="headerlink" title="2:answer construction"></a>2:answer construction</h3><p> 现在我们需要知道我们的预测词和label之间的关系，并且我们也需要填入[z]，他不能是随意的一个单词，所以我们需要一个映射函数(mapping function)：将输出的词与label映射。以上例来看，我们的label是‘笑脸’和‘哭脸’，那么我们可以设置预测词fantastic对应笑脸，boring对应哭脸。</p><h3 id="3-answer-prediction"><a href="#3-answer-prediction" class="headerlink" title="3:answer prediction"></a>3:answer prediction</h3><p> 到了这边我们就只需要选择合适的预训练语言模型，然后进行mask slots [z] 的预测。上例会得到结果fantastic，将其带入[z]中。</p><h3 id="4-answer-label-mapping"><a href="#4-answer-label-mapping" class="headerlink" title="4:answer-label mapping"></a>4:answer-label mapping</h3><p> 最后，对于得到的<em><strong>answer</strong></em>，我们需要使用<em><strong>verbalizer</strong></em>将其映射回原本的label。</p><p> 如：fantastic映射回label笑脸。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><table><thead><tr><th align="center">Terminology</th><th align="center">Notation</th><th align="center">Example</th></tr></thead><tbody><tr><td align="center">Input</td><td align="center">x</td><td align="center">I love this movie</td></tr><tr><td align="center">Output(label)</td><td align="center">y</td><td align="center">😀😭</td></tr><tr><td align="center">Template</td><td align="center">-</td><td align="center">[x] Overall, it was a [z] movie</td></tr><tr><td align="center">Prompt</td><td align="center">x’</td><td align="center">I love this movie.Overall, it  was a [z] movie</td></tr><tr><td align="center">Answer</td><td align="center">z</td><td align="center">fantastic, boring</td></tr></tbody></table><hr>]]></content>
      
      
      <categories>
          
          <category> Pretrain model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Pretrain </tag>
            
            <tag> Prompt Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建</title>
      <link href="/2022/09/09/bo-ke-da-jian/"/>
      <url>/2022/09/09/bo-ke-da-jian/</url>
      
        <content type="html"><![CDATA[<p>本篇介绍使用git、hexo等搭建属于自己的博客方法</p><h2 id="拥有一个git账户"><a href="#拥有一个git账户" class="headerlink" title="拥有一个git账户"></a>拥有一个git账户</h2><p>首先，你需要拥有一个git账户！个人感觉Gitee的稳定性不如GitHub，所以本人使用GitHub作为平台。<a href="https://github.com/">这里是官网</a></p><h2 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h2><p>我选择了<strong>Hexo</strong>框架，作为静态的框架，无论从稳定性还是速度上来看都比较优秀。并且他的使用较为广泛，在各大网站上都可以找到对应的教程和相关问题解决方法，比较适合初学者的开发使用。</p><h2 id="绑定GitHub"><a href="#绑定GitHub" class="headerlink" title="绑定GitHub"></a>绑定GitHub</h2><p>安装<strong>git</strong>（其实本人在使用过程中使用cmd更多）</p><p>git中输入ssh-keygen -t rsa 命令运行，表示我们指定 RSA 算法生成密钥，然后敲四次回车键，之后就就会生成两个文件，分别为秘钥 id_rsa 和公钥 id_rsa.pub.</p><p>然后我吗直接再git bash上打开公钥：</p><pre><code>    $ cd ~/.ssh     $ ls    $ cat id_rsa.pub</code></pre><p>将公钥复制下来。</p><p>然后我们进入GitHub主页如下操作：<br><img src="3.png"><br>点击<strong>SSH and GPG keys</strong>，再点击<strong>New SSH key.</strong><br><img src="4.png"><br>将复制好的公钥粘贴到key内，再点击<strong>Add SSH key</strong>。</p><p>然后我们再git bash上验证是否成功：<br><code>ssh -T git@github.com</code></p><h1 id="搭建Hexo"><a href="#搭建Hexo" class="headerlink" title="搭建Hexo"></a>搭建Hexo</h1><p>这里需要安装<strong>nodejs</strong>。</p><h2 id="安装nodejs"><a href="#安装nodejs" class="headerlink" title="安装nodejs"></a>安装nodejs</h2><p><a href="https://nodejs.org/zh-cn/download/">Nodejs官网</a>下载安装nodejs，选择安装路径，一路next。<br>安装好后cmd中查看安装情况：<br><code>   node -v</code><br><code>   npm -v</code><br>到这里如果没问题，需要<strong>设置npm在安装全局模块时的路径和环境变量</strong>：</p><p>在nodejs文件夹中新建node_cache、node_global两个空文件夹。</p><p><img src="1.png" alt="new folder"><br>cmd内输入命令（我安装的位置在E盘，根据自己的安装路径修改）：</p><pre><code>    npm config set prefix &quot;E:\nodejs\node_global&quot;    npm config set cache &quot;E:\nodejs\node_cache&quot;</code></pre><p>然后设置环境变量，在系统变量中添加”NODE_PATH”，值为“E:\nodejs\node_global\node_modules”；<br>之后再编辑用户变量里的PATH：添加E:\nodejs\node_global。</p><p><strong>cmd测试一下</strong>：<br><code>npm install webpack -g</code></p><h2 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h2><p>首先再Git上创建仓库，格式为<strong>你的用户名.github.io</strong>，权限为<strong>public</strong><br><img src="2.png"></p><p>接下来安装Hexo，首先新建文件夹Blog，然后在此文件夹中右键git bush here启动git，或直接管理员身份运行cmd打开到此文件夹下，输入你npm命令：</p><p><code> npm install -g hexo-cli</code></p><p>初始化：</p><p><code> hexo init</code></p><p>静态部署：</p><p><code> hexo g</code></p><p>在本地查看网页：</p><p><code> hexo s</code><br>然后在浏览器中输入<a href="http://localhost:4000/">http://localhost:4000</a> 打开部署网页<br>输入<strong>ctrl c</strong>停止运行</p><h2 id="Hexo链接到GitHub"><a href="#Hexo链接到GitHub" class="headerlink" title="Hexo链接到GitHub"></a>Hexo链接到GitHub</h2><p>现在在Blog文件夹下打开_config.yml 文件，并添加如下内容：</p><pre><code>deploy:  type: git  repository: git@github.com:yourusername/yourusername.github.io.git  #你的仓库地址  branch: main</code></pre><p><strong>注意：repository可以在git上code中的SSH中复制,每一项后都有空格，如</strong>：<br><code>type: git #type后有一个空格</code></p><p>然后执行命令：<br><code>npm install hexo-deployer-git --save</code><br>再输入三条命令：</p><pre><code>hexo clean   #清除缓存文件 db.json 和已生成的静态文件 publichexo g       #生成网站静态文件到默认设置的 public 文件夹(hexo generate 的缩写)hexo d       #自动生成网站静态文件，并部署到设定的仓库(hexo deploy 的缩写)</code></pre><p>打开浏览器输入<a href="https://xxx.github.io/">https://xxx.github.io</a> 即可访问你的网站。</p><hr><p>如果你想加入域名，那么你需要购买一个域名，详情请看如下博客：</p><p><a href="https://zhuanlan.zhihu.com/p/103860494">购买域名</a></p><p><a href="https://zhuanlan.zhihu.com/p/103813944">解析域名</a></p><hr>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> 博客 </tag>
            
            <tag> GitHub </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>研究学习方法</title>
      <link href="/2022/09/09/yan-jiu-xue-xi-fang-fa/"/>
      <url>/2022/09/09/yan-jiu-xue-xi-fang-fa/</url>
      
        <content type="html"><![CDATA[<h2 id="关于研究学习的方法"><a href="#关于研究学习的方法" class="headerlink" title="关于研究学习的方法"></a>关于研究学习的方法</h2><p>包括论文学习、论文攥写、实验等要求：<br><img src="study.png" alt="研究学习方法"></p><hr>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A brand new life</title>
      <link href="/2022/09/08/a-brand-new-life/"/>
      <url>/2022/09/08/a-brand-new-life/</url>
      
        <content type="html"><![CDATA[<h2 id="一个全新的生活！"><a href="#一个全新的生活！" class="headerlink" title="一个全新的生活！"></a>一个全新的生活！</h2><p>经历了22年的时光，现已结束了本科的学习生活，步入了研究生阶段。开启了人生新的时期！<br>作为一个NLPer，博客主要记录我的论文阅读工作、小组学习、技术以及生活记录等等。<br>希望接下来的三年里，我的研究工作顺利进行！</p><hr>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
