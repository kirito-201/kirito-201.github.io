<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>kirito201</title>
  
  <subtitle>huaya2022</subtitle>
  <link href="http://kirito201.com/atom.xml" rel="self"/>
  
  <link href="http://kirito201.com/"/>
  <updated>2022-11-07T10:37:12.663Z</updated>
  <id>http://kirito201.com/</id>
  
  <author>
    <name>huaya2022</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>极大团查找做不连续NER</title>
    <link href="http://kirito201.com/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/"/>
    <id>http://kirito201.com/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/</id>
    <published>2022-10-25T12:34:19.000Z</published>
    <updated>2022-11-07T10:37:12.663Z</updated>
    
    <content type="html"><![CDATA[<h1 id="极大团查找做不连续NER"><a href="#极大团查找做不连续NER" class="headerlink" title="极大团查找做不连续NER"></a>极大团查找做不连续NER</h1><p>Discontinuous Named Entity Recognition as Maximal Clique Discovery</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>以往的基于转移和基于联合的方法，在训练时都依赖黄金标注的中间结果，使用第一阶段的结果进行预测，产生暴露偏差。</p><p>本文提出了一种将不连续NER转换为寻找图的最大团的非参数化过程，避免暴露偏差。</p><p><strong>极大团定义：当 G′ 是图 G 的子图，且 G′ 是关于 V′ 的完全图时，子图 G’ 为图 G 的团；当 G’ 是团，且不是其他团的子集时，G’ 为图 G 的极大团；当 G’ 是极大团时，且点数最多，G’ 为图 G 最大团</strong></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在处理不连续实体识别上，解决overlap问题设计的方法。</p><p>目前针对不连续的sota的两个方向：combination-based 和 transition-based.（基于联合和基于转移）</p><p>combination-based：首先检测所有重叠的span，然后学习用单独的分类器组合这些段。在训练过程中使用golden段指导分类器，而推理时输入由训练好的模型给出。</p><p>transition-based：通过一连串的移位-还原动作对不连续的span进行增量标记。在训练时当前的动作依赖于之前的golden动作，而测试时整个动作序列由模型生成。</p><p>尽管这些方法的效果有目共睹，但是存在一个共同的问题：<strong>exposure bias</strong></p><p>为解决上述问题，本文提出了Mac（Maximal clique）模型:</p><p><strong>所有句子的mention实体形成一个segment graph，其中的连续段视为节点，同一实体的各部分连接起来形成边。</strong></p><p>那么从这样的segment graph中找到最大团就是问题所在了。</p><p>将这个工作抽象为两个不耦合的部分：segment extraction (SE) and edge prediction (EP)（段提取和边预测）</p><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>下图是抽取过程的一个例子。POB(Part of Body)-身体一部分;ADE(Adverse Drug Event )-不良药物反应</p><p>从句子中抽取实体进行标记分类，然后构建成段图(segment graph)。</p><p><img src="/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/1.PNG" alt></p><h3 id="Grid-Tagging-Scheme"><a href="#Grid-Tagging-Scheme" class="headerlink" title="Grid Tagging Scheme"></a>Grid Tagging Scheme</h3><p>给定一个n-token句子，通过穷举所有可能token对并根据他们的关系标注标签来构建n*n的标签表。</p><h3 id="Segment-Extraction"><a href="#Segment-Extraction" class="headerlink" title="Segment Extraction"></a>Segment Extraction</h3><p>为了提取重叠实体片段，构建一个二维标签表，舍弃下三角区。如图3为一个具体例子。</p><p>若ti到tj的一个段属于一个类别，我们将（ti，tj）的一个段分配一组标签。</p><p>那么实际上，使用BIS标记方法表示段是连续实体（X-S）、位于X类型的不连续实体开头（X-B）或内部（X-I）。</p><p>图中(upper, body)被标记为（POB-S）表示连续实体，类型为“POB”；</p><p>(Sever, joint)被标记为（ADE-B）（Sever joint pain的不连续实体）；(Sever, Sever)-&gt;（ADE-B）；</p><p>( joint, joint)-&gt;（POB-S）.可以看到，解决了重叠实体抽取的问题。</p><p><img src="/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/2.PNG" alt></p><h3 id="Edge-Prediction"><a href="#Edge-Prediction" class="headerlink" title="Edge Prediction"></a>Edge Prediction</h3><p>通过调整同一实体的边界token来构建他们之间的链接进行边预测。</p><p>标签模式定义如下：</p><ul><li>head to head(X-H2H)，则有（ti，tj），ti和tj分别表示构成X类型的一个实体的两个段的开始</li><li>tail to tail (X-T2T)，与上述相似但表示尾部</li></ul><p>在图4中，“Sever”对于“shoulder”和“pain”有头尾两个标签，是由于不连续实体“Sever shoulder pain”为ADE类型实体。</p><p><img src="/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/3.PNG" alt></p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><img src="/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/4.PNG" alt></p><p>使用B-K回溯算法寻找图中最大团。</p><p>构造三个集合：</p><ul><li>R:记录当前极大团中已加入的点</li><li>P:记录可能还可以加入的点（也就是与R集合中所有点都有边存在的点，这样加入后，才会构成团）</li><li>X:记录已经加入过某极大团的点（作用是判重，因为会从每个结点开始，枚举所有的团，如果不对已经加入某个极大团的点，进行标记，可能会有重复的极大团出现）</li></ul><p>那么B-K回溯算法：</p><ol><li>我们将每一个在P中的点v加入集合R，然后更新P，使P中的点依旧可以和R中的点相连接。</li><li>回溯时将V从P中移除，加入X中表示当前状态下对包含V的极大团计算结束。</li><li>R为极大团时，必须P与X都为空。P存放可能加入R的点，P空后没有点可以加入R，X中的点必然可以与R构成极大团。因此当且仅当P、X都为空集时R为一个极大团。</li></ol><p>对于n个token的句子，进行编码，生产2个表达hs和he分别表示SE和EP的特征：</p><p>​                                                            <img src="/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/8.PNG" alt></p><p>其中W为参数矩阵，b为偏移量。</p><h4 id="Segment-Extractor"><a href="#Segment-Extractor" class="headerlink" title="Segment Extractor"></a>Segment Extractor</h4><p><img src="/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/9.PNG" alt></p><p>b和e分别为token的开始和结束，在本模型中固定了行始标记，所以b=ti概率为1只需计算前者。</p><p>对于段提取，使用Conditional Layer Normalization(CLN)做归一化：</p><p><img src="/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/10.PNG" alt></p><p>加入内部信息使用LSTM,以及开始结尾的距离信息：</p><p><img src="/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/11.PNG" alt></p><h4 id="Edge-Predictor"><a href="#Edge-Predictor" class="headerlink" title="Edge Predictor"></a>Edge Predictor</h4><p>无需距离信息和token内部信息，因此：</p><p><img src="/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/12.PNG" alt></p><hr><p>实际上，本文模型的网格标记是一种多标签分类方法，使用Sigmoid做全连接。</p><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><p>三个数据集：CADEC,ShARe 13，ShARe 14</p><p><img src="/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/5.PNG" alt></p><hr><p><img src="/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/6.PNG" alt></p><hr><p><img src="/2022/10/25/ji-da-tuan-cha-zhao-zuo-bu-lian-xu-ner/7.PNG" alt></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;极大团查找做不连续NER&quot;&gt;&lt;a href=&quot;#极大团查找做不连续NER&quot; class=&quot;headerlink&quot; title=&quot;极大团查找做不连续NER&quot;&gt;&lt;/a&gt;极大团查找做不连续NER&lt;/h1&gt;&lt;p&gt;Discontinuous Named Entity Rec</summary>
      
    
    
    
    <category term="Discontinuous NER" scheme="http://kirito201.com/categories/Discontinuous-NER/"/>
    
    
    <category term="NLP" scheme="http://kirito201.com/tags/NLP/"/>
    
    <category term="NER" scheme="http://kirito201.com/tags/NER/"/>
    
    <category term="Discontinuous" scheme="http://kirito201.com/tags/Discontinuous/"/>
    
  </entry>
  
  <entry>
    <title>Transition based</title>
    <link href="http://kirito201.com/2022/10/20/yi-chong-ji-yu-guo-du-de-gao-xiao-de-bu-lian-xu-ming-ming-shi-ti-shi-bie-mo-xing/"/>
    <id>http://kirito201.com/2022/10/20/yi-chong-ji-yu-guo-du-de-gao-xiao-de-bu-lian-xu-ming-ming-shi-ti-shi-bie-mo-xing/</id>
    <published>2022-10-20T09:06:00.000Z</published>
    <updated>2022-10-20T09:10:14.860Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一种基于过渡的高效的不连续命名实体识别模型"><a href="#一种基于过渡的高效的不连续命名实体识别模型" class="headerlink" title="一种基于过渡的高效的不连续命名实体识别模型"></a>一种基于过渡的高效的不连续命名实体识别模型</h1><p>原文：An Effective Transition-based Model for Discontinuous NER</p><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>与通用域中广泛使用的命名实体识别数据集不同，生物医学NER数据集通常包含由不连续跨度组成的mention。常规序列标记技术对有效的马尔可夫假设进行编码，但排除了这些mention。作者为不连续的NER提出了一个简单，有效的基于过渡的模型，并带有通用神经编码。通过对三个生物医学数据集的广泛实验，并且证明了模型可以有效地识别不连续的mention，而不会牺牲连续mention的准确性。</p><ol><li>提出目前非连续实体识别方法缺失，且非连续实体在如医学实体数据集上十分常见；</li><li>提出了Transition-based模型</li></ol><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>首先，flat model的两种假设：</p><ul><li>mentions不是嵌套或重叠的，每个token至多属于一个mention</li><li>mentions由连续的token组成</li></ul><p>相比于连续mentions，非连续mentions的内部间隔很长，如果穷举则增长为指数级；存在时间复杂度过高和解码mentions有歧义的问题。</p><h2 id="Motivations"><a href="#Motivations" class="headerlink" title="Motivations"></a>Motivations</h2><ul><li>非连续实体识别强调组合概念，有别于单独的概念</li><li>在识别非连续mentions时的意义要比识别独立的mentions更有用</li></ul><p>如图：</p><p><img src="1.PNG"></p><p>[^图1]: ‘left atrium dilated’ describes a disorder which has its own CUI in UMLS,whereas both ‘left atrium’ and ‘dilated’ also have.</p><ul><li>具有部分重叠的特点，几个mentions共享同一部分</li><li>重点在将重叠mention分离，避免识别成单个mention</li></ul><h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><ul><li>提出了end-to-end的transition-based模型，用指定的action与attention机制去判断一个span是否是非连续mention的一部分</li><li>在三个非连续医学数据集上评估模型，可以有效识别非连续mention并不影响连续mention的效果。</li></ul><h2 id="Prior-Work"><a href="#Prior-Work" class="headerlink" title="Prior Work"></a>Prior Work</h2><ul><li>Token level approach</li><li>Sentence level approach</li><li>NER task as a structured multi-label classification problem</li></ul><p>前两者，通过先预测出mentions的中间表达（分别基于序列标注和子图），再解码出最后的mention。然而这两个方法在解码时会产生歧义。</p><p>第三个基于分类器检测组成成分，再合并这些成分形成不连续实体。这种方法灵活但是时间复杂度高。</p><ul><li>Discontinuous NER vs. Nested NER</li></ul><p>嵌套和不连续实体的区别在于前者是一个mention完全包含于另一个mention，后者则是两个mention相互包含，存在overlap。大多现有嵌套实体识别模型是处理完整的包含结构，也就无法使用在不连续实体识别。然而，作者发现可以通过在模式中添加细粒度实体类型进而解决不连续NER任务，将不连续NER转换为嵌套NER任务。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>本文模型基于shift-reduce解析器，用stack存储处理过的spans，用buffer存储未处理的tokens。学习框架是：给定parser状态，预测改变解析器状态的操作，循环至结束。</p><p>与其他解析器方法不同的是，本文设计了一组专用于识别不连续实体的一系列action，共6步：</p><ul><li>shift：将buffer中第一个token移入stack；暗示这个token是实体mention的一部分</li><li>out：将buffer栈顶出栈，表示它不属于任何mention</li><li>complete：将stack栈顶span弹出，表示为一个实体</li><li>reduce：将stack出栈两个span s0，s1然后拼接成一个新的span放回stack</li><li>left-reduce：类似于reduce，区别是将s1留在stack中，表示s1涉及多个mentions</li><li>right-reduce：和left-reduce一样，区别在于将s0留在stack中</li></ul><p><img src="2.PNG"></p><ol><li><h3 id="Representation-of-the-Parser-State"><a href="#Representation-of-the-Parser-State" class="headerlink" title="Representation of the Parser State"></a>Representation of the Parser State</h3><p>通过双向LSTM表示第i个token：</p><p><img src="3.PNG"></p><p>使用ELMo的上下文向量拼接词：</p><p><img src="4.PNG"></p><p>这里的C则为buffer中token的状态。</p><p>stack中的span使用stack-LSTM结构从buffer转移到stack的span表示为：</p><p><img src="5.PNG"></p><p>reduce，span拼接操作：</p><p><img src="6.PNG"></p></li><li><h3 id="multiplicative-attention"><a href="#multiplicative-attention" class="headerlink" title="multiplicative attention"></a>multiplicative attention</h3><p>stack中的spans和buffer中的tokens间有对不连续实体识别的重要因素：二者间的关系。</p><p>为了表示这种关系，使用multiplicative attention 。</p><p><img src="7.PNG"></p><p>用stack中的span与buffer中token做关联，得到加权和。</p></li><li><h3 id="select-an-action"><a href="#select-an-action" class="headerlink" title="select an action"></a>select an action</h3><p>parser的表示：（s0,s1,s2,sa0,sa1,sa2）</p><p>分别为stack栈顶三项，以及对应的关联注意力值。</p><p>然后使用单项LSTM学习action，再由softmax预测下一个action。</p></li></ol><h2 id="Data-sets"><a href="#Data-sets" class="headerlink" title="Data sets"></a>Data sets</h2><p><strong>CADEC</strong>，<strong>ShARe 13，ShARe 14</strong></p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p><img src="8.PNG"></p><p><img src="9.PNG"></p><p>基于图模型的准确率依然较高，但是召回率低，本文方法召回率、f1较高，准确性也没有被牺牲。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在不牺牲连续mention准确率的前提下对不连续实体进行识别。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一种基于过渡的高效的不连续命名实体识别模型&quot;&gt;&lt;a href=&quot;#一种基于过渡的高效的不连续命名实体识别模型&quot; class=&quot;headerlink&quot; title=&quot;一种基于过渡的高效的不连续命名实体识别模型&quot;&gt;&lt;/a&gt;一种基于过渡的高效的不连续命名实体识别模型&lt;/</summary>
      
    
    
    
    <category term="Discontinuous NER" scheme="http://kirito201.com/categories/Discontinuous-NER/"/>
    
    
    <category term="NLP" scheme="http://kirito201.com/tags/NLP/"/>
    
    <category term="NER" scheme="http://kirito201.com/tags/NER/"/>
    
    <category term="Discontinuous" scheme="http://kirito201.com/tags/Discontinuous/"/>
    
  </entry>
  
  <entry>
    <title>Bert</title>
    <link href="http://kirito201.com/2022/09/09/bert/"/>
    <id>http://kirito201.com/2022/09/09/bert/</id>
    <published>2022-09-09T11:27:18.000Z</published>
    <updated>2022-09-09T13:45:33.227Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是Bert？"><a href="#什么是Bert？" class="headerlink" title="什么是Bert？"></a>什么是Bert？</h1><p>BERT（Bidirectional Encoder Representations from Transformers） 是一个语言表示模型(language representation model)。它的主要模型结构是trasnformer的encoder堆叠而成，它的框架分为两个步骤：pretraining和fine-tuning。</p><h1 id="Bert可以做什么？"><a href="#Bert可以做什么？" class="headerlink" title="Bert可以做什么？"></a>Bert可以做什么？</h1><p>首先结论是：Bert有分词输出和序列最后一层hidden的输出。因此Bert的应用在分类任务、回归任务和序列任务中。其中回归任务是一种特殊的分类，只不过它的输出是一个数值而非一个概率。序列任务有命名实体识别(NER)，SQuAD。</p><p>Bert的这种预训练模型使得模型只需要少数的训练数据即可得到优秀的效果。</p><h1 id="Bert模型"><a href="#Bert模型" class="headerlink" title="Bert模型"></a>Bert模型</h1><p>简单来讲Bert分为：输入层，中间层，输出层。</p><h2 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h2><p><img src="1.png"><br>可以看到输入为三个embedding相加。</p><h2 id="中间层"><a href="#中间层" class="headerlink" title="中间层"></a>中间层</h2><p>中间层类似于transformer的encoder<br><img src="2.png"></p><h2 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h2><p>每一个输入对应一个输出，根据我们的不同任务来选择具体的输出。</p><h2 id="Pretraining＋fine-tuning"><a href="#Pretraining＋fine-tuning" class="headerlink" title="Pretraining＋fine-tuning"></a>Pretraining＋fine-tuning</h2><p>我们已经知道Bert的两阶段工作性质了，那么它具体如何工作呢？</p><p><strong>- Pretraining</strong><br>      模型的预训练采用大量的预训练数据，进行unsupervised learning。<br>      Bert采用了MLM的方法：mask language model，随机mask输入的词，由于fine-tuning时       没有mask的token，因此：<br>      1. 80%替换为MASK<br>      2. 10%替换为随机token<br>      3. 10%不变<br><strong>- fine-tuning</strong><br>       在fine-tuning阶段可以简单的选择特定的输入和输出进行训练，如：<br>       - 2句子 pairs： 相似度任务,<br>       - 假设-前提 pairs： 推理任务,<br>       - 问题-文章 pairs ： QA任务</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;什么是Bert？&quot;&gt;&lt;a href=&quot;#什么是Bert？&quot; class=&quot;headerlink&quot; title=&quot;什么是Bert？&quot;&gt;&lt;/a&gt;什么是Bert？&lt;/h1&gt;&lt;p&gt;BERT（Bidirectional Encoder Representations fro</summary>
      
    
    
    
    <category term="Pretrain model" scheme="http://kirito201.com/categories/Pretrain-model/"/>
    
    
    <category term="NLP" scheme="http://kirito201.com/tags/NLP/"/>
    
    <category term="Pretrain" scheme="http://kirito201.com/tags/Pretrain/"/>
    
    <category term="Bert" scheme="http://kirito201.com/tags/Bert/"/>
    
  </entry>
  
  <entry>
    <title>Prompt Learning</title>
    <link href="http://kirito201.com/2022/09/09/prompt-learning/"/>
    <id>http://kirito201.com/2022/09/09/prompt-learning/</id>
    <published>2022-09-09T11:15:16.000Z</published>
    <updated>2022-09-09T13:37:35.684Z</updated>
    
    <content type="html"><![CDATA[<p>相关论文：Pre-train, Prompt, and Predict:A Systematic Survey of Prompting Methods in Natural Language Processing</p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>在很长一段时间里，NLP任务采用<strong>Pretrain＋fine-tuning</strong>的方法，这种方法的问题是，对于每个NLP任务我们均需要重新fine-tuning一个新的model。这样来看，它的使用效率并不是很高：对特定任务有效，不能够互相共用。那么，是否能够有一个<strong>预训练模型作为所有任务的基础，仅仅需要将不同任务配适到这个预训练模型上来完成下游任务</strong>？</p><p>那么<strong>Prompt Learning</strong>就是上述的配适器，有了它就可以进行高效的预训练模型使用了。<br><img src="1.png"></p><p>如图所示：</p><ul><li>左侧为Model Tuning：对于不同的任务需要将整个预训练模型调整，拥有自己独立的参数。</li><li>右侧为Prompt Tuning：对于不同的任务，仅需要插入不同的prompt参数，每个任务独立拥有自己的prompt参数进行训练，而不对预训练模型重新训练，大大缩短了训练时间，提升了模型的使用率。</li></ul><h2 id="Prompt"><a href="#Prompt" class="headerlink" title="Prompt"></a>Prompt</h2><p>什么是prompt呢？</p><p>在NLP中prompt代表了一个针对<strong>预训练模型的提示</strong>，帮助它理解问题。<br><img src="2.png"></p><p>在原文的例子里可以很好地理解：BERT&#x2F;BART&#x2F;ERNIE 均为预训练语言模型，对于人类提出的问题，以及线索，预训练语言模型可以给出正确的答案。</p><p>Prompt的定义：</p><blockquote><p>Prompt is the technique of making better use of the knowledge from the pre-trained model by adding additional texts to the input.<br>Prompt 是一种为了更好的使用预训练语言模型的知识，采用在输入段添加额外的文本的技术。</p></blockquote><h2 id="Prompt的工作"><a href="#Prompt的工作" class="headerlink" title="Prompt的工作"></a>Prompt的工作</h2><ul><li>Prompt 模版（Template）的构造</li><li>Prompt 答案空间映射（Verbalizer）的构造</li><li>文本代入template，并且使用预训练语言模型进行预测</li><li>将预测的结果映射回label。</li></ul><p><img src="3.png"></p><h3 id="1-prompt-construction"><a href="#1-prompt-construction" class="headerlink" title="1:prompt construction"></a>1:prompt construction</h3><p>首先我们需要构建一个模版Template，模版的作用是将输入和输出进行重新构造，变成一个新的带有mask slots的文本，具体如下：</p><ul><li>定义一个模版，包含了2处代填入的slots：[x] 和 [z]</li><li>将[x] 用输入文本代入</li></ul><p> 如：</p><ul><li>输入：x &#x3D; 我喜欢这个电影。</li><li>模版：[x]总而言之，它是一个[z]电影。</li><li>代入（prompting）：我喜欢这个电影。总而言之，它是一个[z]电影。</li></ul><h3 id="2-answer-construction"><a href="#2-answer-construction" class="headerlink" title="2:answer construction"></a>2:answer construction</h3><p> 现在我们需要知道我们的预测词和label之间的关系，并且我们也需要填入[z]，他不能是随意的一个单词，所以我们需要一个映射函数(mapping function)：将输出的词与label映射。以上例来看，我们的label是‘笑脸’和‘哭脸’，那么我们可以设置预测词fantastic对应笑脸，boring对应哭脸。</p><h3 id="3-answer-prediction"><a href="#3-answer-prediction" class="headerlink" title="3:answer prediction"></a>3:answer prediction</h3><p> 到了这边我们就只需要选择合适的预训练语言模型，然后进行mask slots [z] 的预测。上例会得到结果fantastic，将其带入[z]中。</p><h3 id="4-answer-label-mapping"><a href="#4-answer-label-mapping" class="headerlink" title="4:answer-label mapping"></a>4:answer-label mapping</h3><p> 最后，对于得到的<em><strong>answer</strong></em>，我们需要使用<em><strong>verbalizer</strong></em>将其映射回原本的label。</p><p> 如：fantastic映射回label笑脸。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><table><thead><tr><th align="center">Terminology</th><th align="center">Notation</th><th align="center">Example</th></tr></thead><tbody><tr><td align="center">Input</td><td align="center">x</td><td align="center">I love this movie</td></tr><tr><td align="center">Output(label)</td><td align="center">y</td><td align="center">😀😭</td></tr><tr><td align="center">Template</td><td align="center">-</td><td align="center">[x] Overall, it was a [z] movie</td></tr><tr><td align="center">Prompt</td><td align="center">x’</td><td align="center">I love this movie.Overall, it  was a [z] movie</td></tr><tr><td align="center">Answer</td><td align="center">z</td><td align="center">fantastic, boring</td></tr></tbody></table><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;相关论文：Pre-train, Prompt, and Predict:A Systematic Survey of Prompting Methods in Natural Language Processing&lt;/p&gt;
&lt;h1 id=&quot;Motivation&quot;&gt;&lt;a hr</summary>
      
    
    
    
    <category term="Pretrain model" scheme="http://kirito201.com/categories/Pretrain-model/"/>
    
    
    <category term="NLP" scheme="http://kirito201.com/tags/NLP/"/>
    
    <category term="Pretrain" scheme="http://kirito201.com/tags/Pretrain/"/>
    
    <category term="Prompt Learning" scheme="http://kirito201.com/tags/Prompt-Learning/"/>
    
  </entry>
  
  <entry>
    <title>博客搭建</title>
    <link href="http://kirito201.com/2022/09/09/bo-ke-da-jian/"/>
    <id>http://kirito201.com/2022/09/09/bo-ke-da-jian/</id>
    <published>2022-09-09T02:23:57.000Z</published>
    <updated>2022-09-09T07:40:27.607Z</updated>
    
    <content type="html"><![CDATA[<p>本篇介绍使用git、hexo等搭建属于自己的博客方法</p><h2 id="拥有一个git账户"><a href="#拥有一个git账户" class="headerlink" title="拥有一个git账户"></a>拥有一个git账户</h2><p>首先，你需要拥有一个git账户！个人感觉Gitee的稳定性不如GitHub，所以本人使用GitHub作为平台。<a href="https://github.com/">这里是官网</a></p><h2 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h2><p>我选择了<strong>Hexo</strong>框架，作为静态的框架，无论从稳定性还是速度上来看都比较优秀。并且他的使用较为广泛，在各大网站上都可以找到对应的教程和相关问题解决方法，比较适合初学者的开发使用。</p><h2 id="绑定GitHub"><a href="#绑定GitHub" class="headerlink" title="绑定GitHub"></a>绑定GitHub</h2><p>安装<strong>git</strong>（其实本人在使用过程中使用cmd更多）</p><p>git中输入ssh-keygen -t rsa 命令运行，表示我们指定 RSA 算法生成密钥，然后敲四次回车键，之后就就会生成两个文件，分别为秘钥 id_rsa 和公钥 id_rsa.pub.</p><p>然后我吗直接再git bash上打开公钥：</p><pre><code>    $ cd ~/.ssh     $ ls    $ cat id_rsa.pub</code></pre><p>将公钥复制下来。</p><p>然后我们进入GitHub主页如下操作：<br><img src="3.png"><br>点击<strong>SSH and GPG keys</strong>，再点击<strong>New SSH key.</strong><br><img src="4.png"><br>将复制好的公钥粘贴到key内，再点击<strong>Add SSH key</strong>。</p><p>然后我们再git bash上验证是否成功：<br><code>ssh -T git@github.com</code></p><h1 id="搭建Hexo"><a href="#搭建Hexo" class="headerlink" title="搭建Hexo"></a>搭建Hexo</h1><p>这里需要安装<strong>nodejs</strong>。</p><h2 id="安装nodejs"><a href="#安装nodejs" class="headerlink" title="安装nodejs"></a>安装nodejs</h2><p><a href="https://nodejs.org/zh-cn/download/">Nodejs官网</a>下载安装nodejs，选择安装路径，一路next。<br>安装好后cmd中查看安装情况：<br><code>   node -v</code><br><code>   npm -v</code><br>到这里如果没问题，需要<strong>设置npm在安装全局模块时的路径和环境变量</strong>：</p><p>在nodejs文件夹中新建node_cache、node_global两个空文件夹。</p><p><img src="1.png" alt="new folder"><br>cmd内输入命令（我安装的位置在E盘，根据自己的安装路径修改）：</p><pre><code>    npm config set prefix &quot;E:\nodejs\node_global&quot;    npm config set cache &quot;E:\nodejs\node_cache&quot;</code></pre><p>然后设置环境变量，在系统变量中添加”NODE_PATH”，值为“E:\nodejs\node_global\node_modules”；<br>之后再编辑用户变量里的PATH：添加E:\nodejs\node_global。</p><p><strong>cmd测试一下</strong>：<br><code>npm install webpack -g</code></p><h2 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h2><p>首先再Git上创建仓库，格式为<strong>你的用户名.github.io</strong>，权限为<strong>public</strong><br><img src="2.png"></p><p>接下来安装Hexo，首先新建文件夹Blog，然后在此文件夹中右键git bush here启动git，或直接管理员身份运行cmd打开到此文件夹下，输入你npm命令：</p><p><code> npm install -g hexo-cli</code></p><p>初始化：</p><p><code> hexo init</code></p><p>静态部署：</p><p><code> hexo g</code></p><p>在本地查看网页：</p><p><code> hexo s</code><br>然后在浏览器中输入<a href="http://localhost:4000/">http://localhost:4000</a> 打开部署网页<br>输入<strong>ctrl c</strong>停止运行</p><h2 id="Hexo链接到GitHub"><a href="#Hexo链接到GitHub" class="headerlink" title="Hexo链接到GitHub"></a>Hexo链接到GitHub</h2><p>现在在Blog文件夹下打开_config.yml 文件，并添加如下内容：</p><pre><code>deploy:  type: git  repository: git@github.com:yourusername/yourusername.github.io.git  #你的仓库地址  branch: main</code></pre><p><strong>注意：repository可以在git上code中的SSH中复制,每一项后都有空格，如</strong>：<br><code>type: git #type后有一个空格</code></p><p>然后执行命令：<br><code>npm install hexo-deployer-git --save</code><br>再输入三条命令：</p><pre><code>hexo clean   #清除缓存文件 db.json 和已生成的静态文件 publichexo g       #生成网站静态文件到默认设置的 public 文件夹(hexo generate 的缩写)hexo d       #自动生成网站静态文件，并部署到设定的仓库(hexo deploy 的缩写)</code></pre><p>打开浏览器输入<a href="https://xxx.github.io/">https://xxx.github.io</a> 即可访问你的网站。</p><hr><p>如果你想加入域名，那么你需要购买一个域名，详情请看如下博客：</p><p><a href="https://zhuanlan.zhihu.com/p/103860494">购买域名</a></p><p><a href="https://zhuanlan.zhihu.com/p/103813944">解析域名</a></p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本篇介绍使用git、hexo等搭建属于自己的博客方法&lt;/p&gt;
&lt;h2 id=&quot;拥有一个git账户&quot;&gt;&lt;a href=&quot;#拥有一个git账户&quot; class=&quot;headerlink&quot; title=&quot;拥有一个git账户&quot;&gt;&lt;/a&gt;拥有一个git账户&lt;/h2&gt;&lt;p&gt;首先，你需要拥有</summary>
      
    
    
    
    <category term="博客" scheme="http://kirito201.com/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="博客" scheme="http://kirito201.com/tags/%E5%8D%9A%E5%AE%A2/"/>
    
    <category term="Hexo" scheme="http://kirito201.com/tags/Hexo/"/>
    
    <category term="GitHub" scheme="http://kirito201.com/tags/GitHub/"/>
    
  </entry>
  
  <entry>
    <title>A brand new life</title>
    <link href="http://kirito201.com/2022/09/08/a-brand-new-life/"/>
    <id>http://kirito201.com/2022/09/08/a-brand-new-life/</id>
    <published>2022-09-08T14:16:27.000Z</published>
    <updated>2022-09-09T04:03:20.408Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一个全新的生活！"><a href="#一个全新的生活！" class="headerlink" title="一个全新的生活！"></a>一个全新的生活！</h2><p>经历了22年的时光，现已结束了本科的学习生活，步入了研究生阶段。开启了人生新的时期！<br>作为一个NLPer，博客主要记录我的论文阅读工作、小组学习、技术以及生活记录等等。<br>希望接下来的三年里，我的研究工作顺利进行！</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一个全新的生活！&quot;&gt;&lt;a href=&quot;#一个全新的生活！&quot; class=&quot;headerlink&quot; title=&quot;一个全新的生活！&quot;&gt;&lt;/a&gt;一个全新的生活！&lt;/h2&gt;&lt;p&gt;经历了22年的时光，现已结束了本科的学习生活，步入了研究生阶段。开启了人生新的时期！&lt;br&gt;</summary>
      
    
    
    
    <category term="生活" scheme="http://kirito201.com/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
    <category term="生活" scheme="http://kirito201.com/tags/%E7%94%9F%E6%B4%BB/"/>
    
    <category term="博客" scheme="http://kirito201.com/tags/%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
</feed>
